# -*- coding: utf-8 -*-
"""PML_HPO

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ap_WaQslMUdIaGSfZBgipQYrTvq3d_cI
"""

#install and import packages
#!pip install --upgrade scikit-learn
#!pip install pandas
import pandas as pd
import numpy as np
#import matplotlib.pyplot as plt

import sklearn

#print python version for report
#!python ‐‐version
import sys; print(sys.version)

#read and display data
data = pd.read_csv("winequality-red.csv", sep = ";")
#data

#display information about dataset for report
data.columns
print(data.groupby(['quality']).count())

#split into features/target
x = data.drop(columns = ['quality'])
#attemptd normalization at one point but this step yielded lower performance
#x_norm = sklearn.preprocessing.normalize(x, axis=0)
y = data['quality']


#suppress warnings about class imbalances
import warnings
warnings.filterwarnings("ignore")

#import models, packages
from sklearn import linear_model, ensemble
from sklearn.model_selection import cross_val_score
from sklearn import model_selection
import numpy
import math

#install and import packages
#!pip install --upgrade scikit-learn
#!pip install pandas
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sklearn

#suppress warnings about class imbalances
import warnings
warnings.filterwarnings("ignore")

#import models, packages
from sklearn import linear_model, ensemble
from sklearn.model_selection import cross_val_score
from sklearn import model_selection
import numpy

#!pip install scikit-optimize
import skopt
from skopt.space import Real, Categorical, Integer
from skopt import BayesSearchCV

#import grid search and cross_validate
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_validate

ridge_opt = BayesSearchCV(linear_model.RidgeClassifier(),
    {
    "alpha": Real(1.0, 5.0, prior='log-uniform'), 
    "tol": Real(0.0001, 0.1, prior='log-uniform'), 
    "solver": Categorical(["svd", "cholesky", "lsqr", "sparse_cg"]),
    "max_iter": Integer(100, 100000, prior='log-uniform'),
    },
     n_iter=32,
     random_state=0, 
    scoring = "balanced_accuracy"

)

bagging_opt = BayesSearchCV(ensemble.BaggingClassifier(), 
                {"n_estimators" : Integer(100, 10000, prior = 'log-uniform'), 
               "max_samples" : Real(0.01, 1.0, prior='log-uniform'), 
                "max_features" : Real(0.01, 1.0, prior='log-uniform'),
                },
        n_iter=32,
        random_state=0, 
                            scoring = "balanced_accuracy"
                            
)
                            
rf_opt = BayesSearchCV(ensemble.RandomForestClassifier(), 
        {
    "n_estimators" : Integer(100, 10000, prior = 'log-uniform'),
    "criterion" : Categorical(["gini", "entropy", "log_loss"]),
    "max_depth" : Real(2, 10, prior='log-uniform')
        },
        n_iter=32,
        random_state=0, 
                       scoring = "balanced_accuracy"
)

def nested_resampling_bayesian(optimizer, x, y):
     for i in range(0, 3): #3-fold outer cross-validation
        x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.2, random_state=42)
        optimizer.fit(x_train, y_train)
        print(optimizer.score(x_test, y_test))
        #10_fold inner resampling
        cv_results = cross_validate(
           optimizer, x_train, y_train, cv=10, return_estimator=True, scoring = "balanced_accuracy"
        )
        cv_results = pd.DataFrame(cv_results)
        cv_test_scores = cv_results["test_score"]
        #display results
        print(
            "Generalization score with hyperparameters tuning:\n"
            f"{cv_test_scores.mean():.3f} ± {cv_test_scores.std():.3f}"
        )
        print(optimizer.best_estimator_)
        print(optimizer.best_params_)
        print(optimizer.best_score_)
nested_resampling_bayesian(ridge_opt, x, y)
nested_resampling_bayesian(bagging_opt, x, y)
nested_resampling_bayesian(rf_opt, x, y)


#models to compare
models = [linear_model.RidgeClassifier(), ensemble.BaggingClassifier(), ensemble.RandomForestClassifier()]
model_names = ["Ridge Classifier", "Bagging Classifier", "RandomForest Classifier"]

def nested_resampling(m, x, y):
    model_score = []
    for i in range(0, 3): #3-fold outer cross-validation
        x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.2, random_state=42)
        #10-fold inner cross-validation to determine best parameters
        scores = sklearn.model_selection.cross_validate(m, x_train, y_train, cv=10, scoring = "balanced_accuracy", return_estimator=True)
        if math.isnan(max(scores["test_score"])): 
            model_score.append(0)
        else:
            best = list(scores["test_score"]).index(max(scores["test_score"]))
            best_model = scores["estimator"][best]
            #test best performing model on outer test set
            y_pred = best_model.predict(x_test)
            #add to list of best model scores
            model_score.append(sklearn.metrics.balanced_accuracy_score(y_test, y_pred))
    return numpy.array(model_score).mean()

def ridge_hpo(x, y):
    best_ridge = 0
    best_config = []
    for alpha in [1.0, 1.1, 2.0, 5.0]:
        for tol in [.0001, 0.001, 0.01, 0.1]:
            for solver in ["svd", "cholesky", "lsqr", "sparse_cg"]:
                for max_iter in [100, 200, 500, 1000, 10000, None]:
                    m = linear_model.RidgeClassifier(alpha = alpha, tol = tol, solver = solver, max_iter = max_iter)
                    mean_balanced_accuracy = nested_resampling(m, x, y)
                    if mean_balanced_accuracy > best_ridge:
                        best_ridge = mean_balanced_accuracy
                        best_config = [alpha, tol, solver, max_iter]
    print("alpha, tol, solver, max_iter", best_config, "mean score", best_ridge)

def bagging_hpo(x, y):
    best_bagging = 0.0
    best_config = []
    for n_estimators in [100, 500, 1000, 10000]:
        for max_samples in [0.1, 1.0, 2, 5]:
            for max_features in [0.1, 1.0, 2, 5]:
                m = ensemble.BaggingClassifier(n_estimators = n_estimators, max_samples = max_samples, max_features = max_features)
                mean_balanced_accuracy = nested_resampling(m, x, y)
                if mean_balanced_accuracy > best_bagging:
                    best_bagging = mean_balanced_accuracy
                    best_config = [n_estimators, max_samples, max_features]
    print("n_estimators, max_samples, max_features", best_config, "mean score", best_bagging)

def random_forest_hpo(x, y):
    best_rf = 0.0
    best_config = []
    for n_estimators in [100, 500, 1000, 10000]:
        for criterion in ["gini", "entropy", "log_loss"]:
            for max_depth in [None, 2, 3, 5, 10]:
                m = ensemble.RandomForestClassifier(n_estimators = n_estimators, criterion = criterion, max_depth = max_depth)
                mean_balanced_accuracy = nested_resampling(m, x, y)
            if mean_balanced_accuracy > best_rf:
                best_rf = mean_balanced_accuracy
                best_config = [n_estimators, criterion, max_depth]
    print("n_estimators, criterion, max_depth", best_config, "mean score", best_rf)

#ridge_hpo(x, y)

#bagging_hpo(x, y)

#random_forest_hpo(x, y)

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_validate

ridge_param_grid = {"alpha": [1.0, 1.1, 2.0, 5.0],
    "tol": [.0001, 0.001, 0.01, 0.1],
    "solver": ["svd", "cholesky", "lsqr", "sparse_cg"],
    "max_iter": [100, 200, 500, 1000, 10000, None]
}

bagging_param_grid = {"n_estimators" : [100, 500, 1000, 10000],
    "max_samples" : [0.1, 1.0, 2, 5],
    "max_features" : [0.1, 1.0, 2, 5]
}

random_forest_param_grid = {"n_estimators" : [100, 500, 1000, 10000],
    "criterion" : ["gini", "entropy", "log_loss"],
    "max_depth" : [None, 2, 3, 5, 10]
}

def nested_resampling_2(model, param_grid, x, y):
    model_grid_search = GridSearchCV(model, param_grid=param_grid, n_jobs=2, cv=3)
    model_grid_search.fit(x, y)
    cv_results = cross_validate(
        model_grid_search, x, y, cv=10, return_estimator=True, scoring = "balanced_accuracy"
    )

    cv_results = pd.DataFrame(cv_results)
    cv_test_scores = cv_results["test_score"]
    print(
        "Generalization score with hyperparameters tuning:\n"
        f"{cv_test_scores.mean():.3f} ± {cv_test_scores.std():.3f}"
    )
    print(model_grid_search.best_estimator_)
    print(model_grid_search.best_params_)

models = [linear_model.RidgeClassifier(), ensemble.BaggingClassifier(), ensemble.RandomForestClassifier()]
param_grids = [ridge_param_grid, bagging_param_grid, random_forest_param_grid]
#for i in range(len(models)):
#    nested_resampling_2(models[i], param_grids[i], x, y)



#plot confusion matrix to visualize true/false positives/negatives among classes
#fig, ax = plt.subplots(figsize=(10, 5))
#sklearn.metrics.ConfusionMatrixDisplay.from_predictions(best_y_test, best_y_pred, ax=ax)